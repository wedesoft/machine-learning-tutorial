\documentclass[a4paper,twoside,10pt]{article}
\usepackage[left=2.45cm,top=2.52cm,right=1.85cm,bottom=2.95cm]{geometry}

\usepackage{hyperref}

\usepackage{csquotes}

\usepackage[printonlyused]{acronym}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{cancel}

\usepackage{amsmath}
\usepackage{txfonts}

\usepackage{minted}

\usepackage[backend=biber,style=alphabetic,natbib=true]{biblatex}

\addbibresource{bibliography.bib}

\title{Notes to Andrew Ng's Machine Learning Course}
\author{Jan Wedekind}

\begin{document}
\maketitle

\section{Introduction}

\subsection{What is Machine Learning}
These are course notes\footnote{source code here: \url{https://igit.comm.ad.roke.co.uk/jw4/machine-learning-tutorial}} to Andrew Ng's video lecture\citep{andrewng}.
A well-posed learning problem (as defined by Tom Mitchell) is:
\begin{displayquote}
  A computer program is said to \emph{learn} from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
\end{displayquote}

Relevant software: Tensorflow\footnote{most popular machine learning software available at \url{https://www.tensorflow.org/}},
Theano\footnote{machine Learning Software available at \url{http://deeplearning.net/software/theano/}},
and Maxima\footnote{computer algebra system available at \url{http://maxima.sourceforge.net/}}.

\subsection{Supervised Learning}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.4\textwidth]{supervised}
    \caption{Labelled data for supervised learning\label{fig:supervised}}
  \end{center}
\end{figure}
Here are two examples for problems of different learning problems (given \emph{labelled} data as shown in Figure \ref{fig:supervised}):
\begin{itemize}
  \item Regression problem (\emph{continuous} value): estimating house price given size in square feet
  \item Classification problem (\emph{discrete} values): determine whether a tumor is malignant or benign given size of tumor and age of person
\end{itemize}

\subsection{Unsupervised Learning}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.4\textwidth]{unsupervised}
    \caption{Data without labels\label{fig:unsupervised}}
  \end{center}
\end{figure}
When the data is not labelled (see \emph{unlabelled} data in Figure \ref{fig:unsupervised}), one can still apply clustering algorithms to it.
For example Google News clusters groups related news articles into stories.
Another basic but impressive example is separating two audio sources (speaker and a radio) using data from two microphones.

For development Andrew Ng recommends GNU Octave\citep{octave}.

\section{Linear Regression with One Variable}
\subsection{Model Representation}
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{r|r}\toprule
      \multicolumn{1}{c|}{\textbf{Size in feet$^2$ $(x)$}} &
      \multicolumn{1}{c}{\textbf{Price (\$) in 1000's $(y)$}}\\\midrule
      2104 & 460\\
      1416 & 232\\
      1534 & 315\\
       852 & 178\\
      $\vdots$ & $\vdots$\\\bottomrule
    \end{tabular}
    \caption{Table of house size and price\label{tbl:houses}}
  \end{center}
\end{table}
\newcommand{\sxi}{\ensuremath{x^{(i)}}}
\newcommand{\syi}{\ensuremath{y^{(i)}}}
\newcommand{\sumi}{\ensuremath{\displaystyle\sum_{i=1}^m}}
House price depending on house size example (see Table \ref{tbl:houses}): supervised regression problem.
The training set is denoted $(x,y)$ and the $i$th sample is $(\sxi,\syi)$ with $i\in\{1,2,\ldots,m\}$ where $m$ is the number of training samples.

The result of the learning algorithm is a \emph{hypothesis} $h$ which maps $x^\prime$ to an estimated value $y^\prime$.
\emph{E.g.} here the hypothesis is the linear function $h_\theta(x)=\theta_0+\theta_1\,x$.

Linear regression with one variable (the variable here is $x$) can also be called \emph{univariate} linear regression.

\subsection{Cost Function}\label{cha:costfunction}
The aim is to minimize the $h_\theta(x)-y$.
\emph{I.e.} minimizing the cost function (or square error function) $J(\theta_0,\theta_1)$ in the following equation
\begin{equation*}
  \displaystyle\mathop{\operatorname{argmin}}_{\theta_0,\theta_1}\underbrace{\frac{1}{2\,m}\,\sumi\big(h_\theta(\sxi)-\syi\big)^2}_{\eqqcolon J(\theta_0,\theta_1)}
\end{equation*}

\subsection{Cost Function Intuition}
With the simplified hypothesis $h_\theta(x)=\theta_1\,x$ we get
\begin{equation*}
  \theta_1=\mathop{\operatorname{argmin}}_{\theta_1}J(\theta_1)=\mathop{\operatorname{argmin}}_{\theta_1}\frac{1}{2\,m}\,\sumi\big(\theta_1\,\sxi-\syi\big)^2
\end{equation*}
The minimum can be obtained by solving for where the derivative is zero
\begin{equation*}
  \frac{\delta}{\delta\theta_1}J(\theta_1)=\frac{1}{m}\,\sumi\big(\theta_1\,\sxi-\syi\big)\,\sxi\overset{!}{=}0\Leftrightarrow
  \theta_1=\sumi\sxi\,\syi\bigg/\sumi\sxi\,\sxi
\end{equation*}
Using a contour plot one can visualise the two-dimensional version of $J(\theta_0,\theta_1)$ when using the full model.

Least-square estimation for $h_\theta(x)=\theta_0+\theta_1\,x$ yields
\begin{equation*}
\frac{1}{m}\,\sumi\big(\theta_0+\theta_1\,\sxi-\syi\big)\,\begin{pmatrix}1\\\sxi\end{pmatrix}\overset{!}{=}\vec{0}\Leftrightarrow
\begin{pmatrix}\theta_0\\\theta_1\end{pmatrix}=
\begin{pmatrix}m&\sum\sxi\\\sum\sxi&\sum\sxi\,\sxi\end{pmatrix}^{-1}\,
\begin{pmatrix}\sum\syi\\\sum\sxi\,\syi\end{pmatrix}
\end{equation*}
See Appendix \ref{app:lse} for the implementation. Figure \ref{fig:lse} shows sample data with a least square fit.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{least_squares}
    \caption{Example data and corresponding least square fit of a linear function\label{fig:lse}}
  \end{center}
\end{figure}

\subsection{Gradient Descent}\label{cha:gradientdescent}
Starting from an initial solution $\theta_j$ with $j\in\{0,1\}$, the solution is updated iteratively using
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\,\frac{\delta}{\delta\theta_j}J(\theta_0,\theta_1)
\end{equation*}
where $\alpha$ is the learning rate.
Note that the parameters are updated ``simultaneously'' using the gradient vector $\delta J/\delta\theta$
\begin{equation}\label{equ:gradientdescent}
  \theta\coloneqq\theta-\alpha\,\frac{\delta}{\delta\theta}J(\theta)
\end{equation}

\subsection{Gradient Descent Intuition}
\begin{itemize}
  \item If $\alpha$ is too small, the algorithm will converge slowly.
  \item If $\alpha$ is too large, the algorithm will not converge on the local minimum.
\end{itemize}

\subsection{Gradient Descent for Linear Regression}
Gradient descent is performed using Equation (\ref{equ:gradientdescent}). Here the gradient is
\begin{equation*}
  \frac{\delta}{\delta\theta}J(\theta)=
  \frac{1}{m}\,\sumi\big(\theta_0+\theta_1\,\sxi-\syi\big)\,\begin{pmatrix}1\\\sxi\end{pmatrix}=
  \frac{1}{m}\,\begin{pmatrix}m&\sum\sxi\\\sum\sxi&\sum\sxi\,\sxi\end{pmatrix}\,
  \begin{pmatrix}\theta_0\\\theta_1\end{pmatrix}
\end{equation*}

See Appendix \ref{app:gradientdescent} for an implementation using symbolic differentiation of the cost function $J$.
A result of the iterative algorithm is shown in Figure \ref{fig:gradient_descent}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{gradient_descent}
    \caption{Example data and corresponding least square fit of a linear function\label{fig:gradient_descent}}
  \end{center}
\end{figure}

\section{Linear Regression with Multiple Variables}\label{cha:linearmulti}
\subsection{Multiple Features}
In the case of $n$ features, each feature $\sxi$ is a $n+1$ dimensional feature vector with $x_0$ set to $1$ for convenience:
\begin{equation*}
  \sxi=\begin{pmatrix}\sxi_0\\\sxi_1\\\vdots\\\sxi_n\end{pmatrix}\in\mathbb{R}^{n+1}
  \mathrm{\ where\ }\sxi_0\coloneqq 1
\end{equation*}
The hypothesis $h$ is
\begin{equation*}
  h_\theta(x)=\theta_0+\theta_1\,x_1+\theta_2\,x_2+\ldots+\theta_n\,x_n
\end{equation*}
Using $x_0=1$ the hypothesis can be written more compactly using the vector inner product:
\begin{equation*}
  h_\theta(x)=\theta^\top x
\end{equation*}
Regression using multiple variables is called \emph{multivariate regression}.

\subsection{Gradient Descent for Multiple Variables}\label{cha:linearregression}
The cost function $J$ can be written using vector notation
\begin{equation*}
  J(\theta)=\frac{1}{2\,m}\sumi\big(h_\theta(\sxi)-\syi\big)^2
\end{equation*}
As in Section \ref{cha:gradientdescent}, gradient descent is performed using the parameter $\alpha$:
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\cdot\underbrace{\frac{\delta}{\delta\theta_j}J(\theta)}_
  {\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi}\mathrm{\ for\ }j\in\{0,\ldots,n\}
\end{equation*}

\subsection{Feature Scaling}
Feature scaling is about scaling $x_1,x_2,\ldots$ so that the variation of the values is similar.
Gradient descent then will converge more quickly (see Figure \ref{fig:scaling}).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{scaling}
    \caption{Feature scaling improves the performance of gradient descent\citep{andrewng}\label{fig:scaling}}
  \end{center}
\end{figure}
A common approach is to normalise the features to zero mean ($\mu=0$) and a standard deviation of one ($\sigma=1$).

\subsection{Learning Rate}
When performing gradient descent, $J(\theta)$ should decrease after each iteration.
Plotting $J(\theta)$ over each iteration can help to tell whether gradient descent has converged (see Figure \ref{fig:alphas}).
If $J(\theta)$ is increasing, $\alpha$ might be too large.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{learning_rate}
  \caption{Plotting the cost function over the iterations can help choosing an appropriate value for the learning rate $\alpha$\label{fig:alphas}}
  \end{center}
\end{figure}

\subsection{Features and Polynomial Regression}\label{cha:polyregression}
Similar as in support vector machines one can perform linear regression on non-linear functions of features.
\emph{I.e.} given a feature $x$, one can generate additional features $x^2$, $\sqrt x$, $\ldots$.

\subsection{Normal Equation}\label{cha:normal}
An analytical solution for the linear regression problem exists since the cost function is a quadratic function and
can be written as follows
\begin{equation*}
  J(\theta)=a\,\theta^2+b\,\theta+c
\end{equation*}
An analytical solution for minimizing $J$ can be found by solving the following equation system
\begin{equation*}
  \frac{\delta}{\delta\theta_j}J(\theta)=0
\end{equation*}
The least squares problem is written in the following form.
\begin{equation*}
  \underbrace{\begin{pmatrix}
1&x^{(1)}_1&x^{(1)}_2&\hdots&x^{(1)}_n\\
1&x^{(2)}_1&x^{(2)}_2&\hdots&x^{(2)}_n\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x^{(m)}_1&x^{(m)}_2&\hdots&x^{(m)}_n
  \end{pmatrix}}_{\eqqcolon\mathcal{X}}\,\theta=
\underbrace{\begin{pmatrix}y^{(1)}\\\vdots\\y^{(m)}\end{pmatrix}}_{y}+\epsilon
\end{equation*}
The solution is $\theta=(\mathcal{X}^\top\mathcal{X})^{-1}\mathcal{X}^\top y$.

\subsection{Normal Equation Non Invertibility}
There are two cases where the least square solution $\theta=(\mathcal{X}^\top\mathcal{X})^{-1}\mathcal{X}^\top y$ is not invertible
\begin{itemize}
  \item redundant (linear dependent) features
  \item too many features (\emph{e.g.} more features than data, $m\le n$)
    \begin{itemize}
      \item delete some features or
      \item use regularisation
    \end{itemize}
\end{itemize}
The closed solution is preferable if the number of features $n$ is small (see Figure \ref{fig:gcp}).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{gradientproscons}
    \caption{Pros and cons for using gradient descent instead of the normal equation\citep{andrewng}\label{fig:gcp}}
  \end{center}
\end{figure}

\section{Logistic Regression}
\subsection{Classification}
The basic case is the binary classification problem $y\in\{0,1\}$.
Applying linear regression and thresholding the result (\emph{e.g.} $h_\theta(x)\ge 0.5$ generally does not perform well.
$0\le h_\theta(x)\le 1$ is not fullfilled for most linear models.

\subsection{Hypothesis Representation}
The logistic regression model uses the \emph{sigmoid} function (or \emph{logistic} function).
\begin{equation*}
  h_\theta(x)=g(\theta^\top x)\mathrm{\ where\ }g(z)=\frac{1}{1+\operatorname{e}^{-z}}
\end{equation*}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.4\textwidth]{sigmoid}
    \caption{Sigmoid function\label{fig:sigmoid}}
  \end{center}
\end{figure}
Figure \ref{fig:sigmoid} shows the characteristic ``S''-shaped curve of the sigmoid function\footnote{\url{https://en.wikipedia.org/wiki/Sigmoid_function}}.
The model based on the sigmoid function is
\begin{equation*}
  h_\theta(x)=\frac{1}{1+\operatorname{e}^{-\theta^\top x}}
\end{equation*}
The output of $h_\theta(x)$ is interpreted as the estimated probability that $y=1$ on input $x$.
\begin{equation*}
  h_\theta(x)=P(y=1|\,x;\theta)
\end{equation*}

\subsection{Decision Boundary}
The prediction is
\begin{itemize}
  \item ``$y=1$'' if $h_\theta(x)\ge 0.5$
  \item ``$y=0$'' if $h_\theta(x)<0.5$
\end{itemize}
With $h_\theta(x)=g(\theta^\top x)$ and $g$ being monotonous, this is equivalent to
\begin{itemize}
  \item ``$y=1$'' if $\theta^\top x\ge 0$
  \item ``$y=0$'' if $\theta^\top x< 0$
\end{itemize}
The decision boundary in this case is a hyperplane in the feature space. \emph{E.g.} given two features
\begin{equation*}
  h_\theta(x)=g(\theta_0+\theta_1\,x_1+\theta_2\,x_2)
\end{equation*}
the decision boundary is
\begin{equation*}
  \theta_0+\theta_1\,x_1+\theta_2\,x_2=0
\end{equation*}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{decisionboundary}
    \caption[Linear decision boundary]{Linear decision boundary. ``x'' denotes a positive sample (y=1) and ''o'' a negative sample (y=1)\citep{andrewng}\label{fig:decisionboundary}}
  \end{center}
\end{figure}
It separates the ``y=1'' region and the ``y=0'' region (also see Figure \ref{fig:decisionboundary}).

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{circularboundary}
    \caption{Circular decision boundary\citep{andrewng}\label{fig:circularboundary}}
  \end{center}
\end{figure}
Similar as in Section \ref{cha:polyregression} one can use polynomial terms to introduce additional features in order to achieve a non-linear decision boundary (see Figure \ref{fig:circularboundary}). \emph{E.g.} one can use a circular decision boundary in the hypothesis
\begin{equation*}
  h_\theta(x)=g(\underbrace{\theta_0}_{=-1}+\underbrace{\theta_1}_{=0}\,x_1+\underbrace{\theta_2}_{=0}\,x_2+\underbrace{\theta_3}_{=1}\,x_1^2+\underbrace{\theta_4}_{=1}\,x_2^2)
\end{equation*}

\subsection{Cost Function}
Given
\begin{itemize}
\item a training set of $m$ training examples $\{(x^{(1)},y^{(1)},x^{(2)},y^{(2)},\ldots,x^{(m)},y^{(m)}\}$ with
$x\in\begin{pmatrix}x_0\\x_1\\\vdots\\x_n\end{pmatrix}$, $x_0=1$, $y\in\{0,1\}$
\item a hypothesis $h_\theta(x)=\displaystyle\frac{1}{1+\operatorname{e}^{-\theta^\top x}}$
\end{itemize}
Desired is a parameter vector $\theta$ so that $h_\theta(x)$ produces good predictions.

The cost function introduced in Section \ref{cha:costfunction} is generalised as follows
\begin{equation*}
  J(\theta)=\frac{1}{m}\sumi Cost(h_\theta(\sxi),\syi)
\end{equation*}
In the case of linear regression the cost function is $cost(h_\theta(x),y)=\frac{1}{2}(h_\theta(x)-y)^2$.
In the case of logistic regression this function would be ``non-convex''. Instead the following cost function is used
\begin{equation*}
Cost(h_\theta(x),y)=
\left\{\begin{array}{rl}-\operatorname{log}(h_\theta(x))&\mathrm{if\ }y=1\\
-\operatorname{log}(1-h_\theta(x))&\mathrm{if\ }y=0\end{array}\right.
\end{equation*}
\begin{figure}[htbp]
  \begin{center}
    \begin{subfigure}[b]{.47\textwidth}
      \includegraphics[width=\linewidth]{costy1}
      \caption{Cost function for ``y=1''}
    \end{subfigure}
    \begin{subfigure}[b]{.47\textwidth}
      \includegraphics[width=\linewidth]{costy0}
      \caption{Cost function for ``y=0''}
    \end{subfigure}
    \caption{Cost function for logistic regression\citep{andrewng}\label{fig:costlog}}
  \end{center}
\end{figure}
See Figure \ref{fig:costlog} for a visualisation.

\subsection{Simplified Cost Function and Gradient Descent}
The cost function
\begin{equation*}
Cost\big(h_\theta(x),y\big)=
\left\{\begin{array}{rl}-\operatorname{log}\big(h_\theta(x)\big)&\mathrm{if\ }y=1\\
-\operatorname{log}\big(1-h_\theta(x)\big)&\mathrm{if\ }y=0\end{array}\right.
\end{equation*}
can be rewritten as follows
\begin{equation*}
Cost\big(h_\theta(x),y\big)=
-y\,\operatorname{log}\big(h_\theta(x)\big)-(1-y)\,\operatorname{log}\big(1-h_\theta(x)\big)
\end{equation*}
The overall cost function $J(\theta)$ is\footnote{similar as in maximum likelihood estimation}
\begin{equation*}
  J(\theta)=-\frac{1}{m}\big[\sumi\syi\operatorname{log}h_\theta(\sxi)+(1-\syi)\operatorname{log}(1-h_\theta(\sxi))\big]
\end{equation*}

The minimization problem $\mathop{\operatorname{argmin}}_\theta J(\theta)$ can be solved using gradient descent
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\cdot\underbrace{\frac{\delta}{\delta\theta_j}J(\theta)}_
  {=\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi}
\end{equation*}
Note that the choice of $h_\theta(x)=\frac{1}{1+\operatorname{e}^{-\theta^\top x}}$ for logistic regression has led to the \emph{same} gradient descent rule as used for linear regression (with $h_\theta(x)=\theta^\top x$ as shown in Section \ref{cha:linearregression}).

See Figure \ref{fig:classifier} for an example of classifying binary data with two features.
The corresponding implementation is shown in Appendix \ref{app:classifier}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{classifier}
    \caption{Classifying binary data with two features\label{fig:classifier}}
  \end{center}
\end{figure}

Figure \ref{fig:polynomial} shows an example using polynomial (non-linear) features (see Appendix \ref{app:polynomial} for implementation).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{polynomial}
    \caption{Classifying binary data using polynomial features\label{fig:polynomial}}
  \end{center}
\end{figure}

\subsection{Advanced Optimization}
There are other algorithms for optimizing the cost functions given
\begin{itemize}
  \item the cost function $J(\theta)$
  \item the derivative $\frac{\delta}{\delta\theta_j}J(\theta)$
\end{itemize}
Some optimization algorithms are
\begin{itemize}
  \item Gradient descent
  \item Conjugate gradient
  \item \acs{BFGS}\footnote{\url{https://en.wikipedia.org/wiki/Broyden\%E2\%80\%93Fletcher\%E2\%80\%93Goldfarb\%E2\%80\%93Shanno_algorithm}}
  \item \acs{L-BFGS}\footnote{\url{https://en.wikipedia.org/wiki/Limited-memory_BFGS}}
\end{itemize}
More advanced algorithms have the following advantages
\begin{itemize}
  \item no need to manually pick $\alpha$
  \item often converge faster than gradient descent
\end{itemize}
However the algorithms are more complex. It is not recommended to implement this algorithms yourself.

\subsection{Multi-Class Classification: One-vs-all}\label{cha:onevsall}
The binary classifier can be generalised to multi-class problems using multiple one-vs-all classifiers.
\emph{E.g.} for $y\in\{1,2,3\}$ three classifiers $h_\theta^{(i)}(x)=P(y=i\|\,x;\theta)$ for $i\in\{1,2,3\}$ are trained.
To make a prediction, the classifier outputting the highest probability is selected (see Figure \ref{fig:onevsall}):
\begin{equation*}
  \mathop{\operatorname{max}}_ih_\theta^{(i)}(x)
\end{equation*}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{onevsall}
    \caption{Multi-class classification using one-vs-all classifiers\citep{andrewng}\label{fig:onevsall}}
  \end{center}
\end{figure}

\section{Regularization}\label{cha:regularization}
\subsection{The Problem of Overfitting}
\begin{itemize}
  \item If the model is \textbf{underfit}, there is a poor fit to the data causing a \emph{high bias}.
  \item The other extreme is an \textbf{overfit} model with too many parameters causing high \emph{high variance}.
\end{itemize}
If the data is overfit, the fit is very well (low value for $J(\theta)$) but it fails to generalize to new examples.
See Figure \ref{fig:overfitting} for an eample of overfitting (implementation in Appendix \ref{app:overfitting}).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{overfitting}
    \caption{Example of overfitting\label{fig:overfitting}}
  \end{center}
\end{figure}

One can address overfitting by
\begin{enumerate}
  \item reduce number of features
    \begin{itemize}
      \item manually select features to keep
      \item model selection algorithm
    \end{itemize}
  \item regularization
    \begin{itemize}
      \item keep all the features, but reduce magnitude/values of parameters $\theta_j$
      \item works well when we have a lot of features, each of which contributes a bit to predicting $y$
    \end{itemize}
\end{enumerate}

\subsection{Cost Function}
The cost function can be modified to penalize large values for parameters.
The cost function is modified as follows
\begin{equation*}
  J(\theta)=\frac{1}{2\,m}\,\bigg[\sumi\big(h_\theta(\sxi)-\syi\big)^2+\underbrace{\lambda\,\sum_{i=1}^n\theta_j^2\bigg]}_{\mathrm{regularization\ term}}
\end{equation*}
where $\lambda$ is the \emph{regularization parameter}.
Note that by convention $\theta_0$ (constant component of the model) is not regularized.
The regularization parameters controls the trade-off between a good model fit and a regular model.

For $\lambda=0$ the cost function is the same as before which is prone to overfitting. For $\lambda\rightarrow\infty$ the resulting hypothesis converges to the constant model $h_\theta(x)=\theta_0$ and underfitting occurs.

\subsection{Regularized Linear Regression}
Applying the gradient descent algorithm to the regularized cost function results in the following update rule
\begin{align*}
  \begin{split}
    \theta_0\coloneqq&\theta_0-\alpha\,\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi_0\\
    \theta_j\coloneqq&\theta_j-\alpha\,\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi_j-\alpha\,\frac{\lambda}{m}\theta_j=\\
    &\theta_j\,(1-\alpha\,\frac{\lambda}{m})-\alpha\,\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi_j
    \mathrm{\ for\ }j\in\{1,2,3,\ldots,n\}
  \end{split}
\end{align*}
The term $(1-\alpha\frac{\lambda}{m})$ equals $1-\epsilon$ with $\epsilon>0$ being a small value which shrinks each parameter towards zero at each step.

The normal equation introduced in Section \ref{cha:normal} becomes
\begin{equation*}
  \theta=\Bigg(\mathcal{X}^\top\mathcal{X}+\lambda\,
\begin{pmatrix}
  0&0&\hdots&0\\
  0&1&\ddots&\vdots\\
  \vdots&\ddots&\ddots&0\\
  0&\hdots&0&1
\end{pmatrix}\Bigg)^{-1}\mathcal{X}^\top y
\end{equation*}
In the case of $m\le n$ (fewer examples than features), $\mathcal{X}^\top\mathcal{X}$ is still invertible.

\subsection{Regularized Logistic Regression}\label{cha:regreg}
In the case of logistic regression the cost function is modified to
\begin{equation*}
  J(\theta)=-\frac{1}{m}\bigg[\sumi\syi\operatorname{log}h_\theta(\sxi)+(1-\syi)\operatorname{log}\big(1-h_\theta(\sxi)\big)\bigg]+
  \frac{\lambda}{2\,m}\sum_{j=1}^n\theta_j^2
\end{equation*}
Note that the bias unit $\theta_0$ is not regularized.

The gradient descent algorithm becomes
\begin{align*}
  \begin{split}
    \theta_0\coloneqq&\theta_0-\alpha\,\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi_0\\
    \theta_j\coloneqq&\theta_j-\alpha\,\underbrace{\bigg[\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\,\sxi_j+\frac{\lambda}{m}\theta_j\bigg]}_{=\frac{\delta}{\delta\theta_j}J(\theta)}
    \mathrm{\ for\ }j\in\{1,2,3,\ldots,n\}
  \end{split}
\end{align*}
Note that the hypothesis here is $h_\theta(x)=\frac{1}{1+\operatorname{e}^{-\theta^\top x}}$.

\section{Neural Networks Representation}
\subsection{Non Linear Hypotheses}
Using polynomial features as shown in Section \ref{cha:polyregression} does not scale well with the number of features.
When applying logistic regression to image processing, each camera pixel is a feature.
\emph{E.g.} a $50\times 50$ image results in 2500 features (or 7500 features when using RGB images).

\subsection{Neurons and the Brain}
Using rewiring and other experiments neurologists have shown that any part of a brain can learn different tasks.
The motivation behind neural networks is to find a generic algorithm which can be applied to a variety of problems.

\subsection{Model Representation}
The neuron is modelled as a logistic unit as shown in Figure \ref{fig:logisticunit}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{logisticunit}
    \caption{Modelling a neuron as a logistic unit\citep{andrewng}\label{fig:logisticunit}}
  \end{center}
\end{figure}
$x_1, x_2, \ldots$ are called ``input units'' and $x_0$ is the ``bias unit''.
$h_\theta(x)$ is the ``output''.
The sigmoid function $g(z)=\frac{1}{1+\operatorname{e}^{-z}}$ is named ``activation function''.
The components of the parameter vector $\theta$ can be called ``weights''.

A \emph{neural network} computes the output from a hidden layer instead (see Figure \ref{fig:hidden}) and can learn its own features.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{hidden}
    \caption{Neural network with a hidden layer\citep{andrewng}\label{fig:hidden}}
  \end{center}
\end{figure}
Additional definitions are introduced
\begin{itemize}
  \item $a^{(j)}_i\in[0,1]$ ``activation'' of unit $i$ in layer $j\ge 2$ (with bias unit $a^{(j)}_0=1$)
  \item $\theta^{(j)}\in\mathbb{R}^{s_{j+1}\times(s_j+1)}$ matrix of weights controlling function mapping from layer $j$ to layer $j+1$
  \item $s_j$ is the number of units in layer $j$
\end{itemize}
When there is one hidden layer, \emph{forward propagation} is performed as follows.
The activations of units in layer $j+1$ are determined using matrix multiplication with $\theta^{(j)}$
\begin{align*}
  \begin{split}
    a^{(2)}_0&=1\\
    a^{(2)}_1&=g(\underbrace{\theta^{(1)}_{10}\,x_1+\theta^{(1)}_{11}\,x_1+\ldots}_{=z^{(2)}_1})\\
    a^{(2)}_2&=g(\underbrace{\theta^{(1)}_{20}\,x_1+\theta^{(1)}_{21}\,x_1+\ldots}_{=z^{(2)}_2})\\
    &\vdots
  \end{split}
\end{align*}
The output is determined from the last hidden layer
\begin{equation*}
  h_\theta(x)=g(\theta^{(2)}_{10}\,a^{(2)}_0+\theta^{(2)}_{11}\,a^{(2)}_1+\theta^{(2)}_{12}\,a^{(2)}_2+\ldots)
\end{equation*}
The definition is made more compact using vectorized expressions
\begin{itemize}
  \item $z^{(2)}=\theta^{(1)}\,x\in\mathbb{R}^{s_2}$
  \item $a^{(2)}=g(z^{(2)})$ with $a^{(2)}_0=1$ ($a^{(2)}\in\mathbb{R}^{s_2+1}$)
\end{itemize}
where the definition of the digmoid function $g$ is extended to work element-wise on vectors and prepend $1$ as the first element.
The output is determined as follows
\begin{equation*}
  h_\theta(x)=a^{(3)}=g(z^{(3)})
\end{equation*}

Essentially a neural network uses more than one logistical regression layer.
Neural networks can have more than three layers. \emph{E.g.} a network with four layers
\begin{itemize}
  \item [Layer 1:] Input
  \item [Layer 2:] Hidden layer
  \item [Layer 3:] Hidden layer
  \item [Layer 4:] Output
\end{itemize}

\subsection{Examples and Intuitions}
Given two inputs (and the bias unit) one can approximate a logical ``and'' operation as follows
\begin{equation*}
  h_\theta(x)=g(-30+20\,x_1+20\,x_2)
\end{equation*}
The result is shown in the following table
\begin{center}
  \begin{tabular}{cc|c}
    $x_1$ & $x_2$ & $h_\theta(x)$\\\hline
    0 & 0 & 0.000\\
    0 & 1 & 0.000\\
    1 & 0 & 0.000\\
    1 & 1 & 1.000\\
  \end{tabular}
\end{center}
In the same fashion one can approximate a logical ``or''
\begin{equation*}
  h_\theta(x)=g(-10+20\,x_1+20\,x_2)
\end{equation*}
The result is shown in the table below
\begin{center}
  \begin{tabular}{cc|c}
    $x_1$ & $x_2$ & $h_\theta(x)$\\\hline
    0 & 0 & 0.000\\
    0 & 1 & 1.000\\
    1 & 0 & 1.000\\
    1 & 1 & 1.000\\
  \end{tabular}
\end{center}
Logical ``not'' can be approximated as follows
\begin{equation*}
  h_\theta(x)=g(10-20\,x_1)
\end{equation*}
Resulting in the desired behaviour as shown in the table below
\begin{center}
  \begin{tabular}{cc|c}
    $x_1$ & $h_\theta(x)$\\\hline
    0 & 1.000\\
    1 & 0.000\\
  \end{tabular}
\end{center}
``(not $x_1$) and (not $x_2$)'' can be implemented using
\begin{equation*}
  h_\theta(x)=g(10-20\,x_1-20\,x_2)
\end{equation*}

``($x_1$ xnor $x_2$)'' (not exclusive-or) requires a hidden layer.
The hidden layer computes the logical ``and'' and the logical ``nor''.
The activations of the hidden layer are combined using a logical ``or''.
See table below for the different states of the neural network
\begin{center}
  \begin{tabular}{cccc|c}
    $x_1$ & $x_2$ & $a^{(2)}_1$ & $a^{(2)}_2$ & $h_\theta(x)$\\\hline
    0 & 0 & 0 & 1 & 1.000\\
    0 & 1 & 0 & 0 & 0.000\\
    1 & 0 & 0 & 0 & 0.000\\
    1 & 1 & 1 & 0 & 1.000\\
  \end{tabular}
\end{center}
\emph{I.e.} to implement ``xnor'' we use
\begin{itemize}
\item $\theta^{(1)}=\begin{pmatrix}-30&20&20\\10&-20&-20\end{pmatrix}$ and
\item $\theta^{(2)}=\begin{pmatrix}-10&20&20\end{pmatrix}$
\end{itemize}
The neural network is visualised in Figure \ref{fig:xnor}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{xnor}
    \caption{Logical ``xnor'' implemented with a neural network\citep{andrewng}\label{fig:xnor}}
  \end{center}
\end{figure}

Neural networks can be used for complex tasks such as handwritten digit classification\citep{lecun} (see Figure \ref{fig:lecun}).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{lecun}
    \caption{Handwritten character recognition using neural network\citep{lecun}\label{fig:lecun}}
  \end{center}
\end{figure}


\subsection{Multi-Class Classification}\label{cha:multiclassif}
Multi-class classification with neural networks is an extension of the one-vs-all method (see Section \ref{cha:onevsall}).
Basically the desired output is specified to be a unit vector.
\emph{E.g.} the desired outputs (and training labels $\syi$) for four classes ($K=4$) are
\begin{itemize}
  \item $h_\theta(x)\approx\begin{pmatrix}1\\0\\0\\0\end{pmatrix}$ for class one
  \item $h_\theta(x)\approx\begin{pmatrix}0\\1\\0\\0\end{pmatrix}$ for class two
  \item $h_\theta(x)\approx\begin{pmatrix}0\\0\\1\\0\end{pmatrix}$ for class three
  \item $\ldots$
\end{itemize}
The desired training result is $h_\theta(\sxi)\approx\syi$ with $\syi\in\mathbb{R}^K$ and $K=4$.

\subsection{Cost Function}
\begin{itemize}
  \item $L$ is the total number of layers of the network
  \item $s_l$ is the number of units in layer $l\in\{1,2,\ldots,L\}$
  \item $K=s_L$ is the number of output units
  \item $\big(h_\theta(x)\big)_i$ is the $i$th component of the vector $h_theta(x)$
\end{itemize}

In Section \ref{cha:regreg} the cost function was
\begin{equation*}
  J(\theta)=-\frac{1}{m}\bigg[\sumi\syi\operatorname{log}h_\theta(\sxi)+(1-\syi)\operatorname{log}(1-h_\theta(\sxi))\bigg]+
  \frac{\lambda}{2\,m}\sum_{j=1}^n\theta_j^2
\end{equation*}
with the bias unit $\theta_0$ not being regularized. The cost function is generalised for neural networks as follows
\newcommand{\sumk}{\ensuremath{\displaystyle\sum_{k=1}^K}}
\newcommand{\suml}{\ensuremath{\displaystyle\sum_{l=1}^{L-1}}}
\begin{equation*}
  J(\theta)=-\frac{1}{m}\bigg[\sumi\sumk\syi\operatorname{log}(h_\theta(\sxi))_k+(1-\syi)\operatorname{log}\big(1-(h_\theta(\sxi))_k\big)\bigg]+
  \frac{\lambda}{2\,m}\suml\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\big(\theta^{(l)}_{ji}\big)^2
\end{equation*}
Note that the bias units $\theta_{j0}$ are not regularized.

\subsection{Backpropagation Algorithm}
In order to find $\mathop{\operatorname{argmin}}_\theta J(\theta)$ we need to compute $\frac{\delta}{\delta\theta^{(l)}_{ij}}J(\theta)$.
Given one training example $(x,y)$ forward propagation is used to determine the activations.
\emph{E.g.} in the case of $L=4$
\begin{itemize}
  \item $a^{(1)}=x$
  \item $z^{(2)}=\theta^{(1)}\,a^{(1)}$
  \item $a^{(2)}=g(z^{(2)})$ (add $a^{(2)}_0$)
  \item $z^{(3)}=\theta^{(2)}\,a^{(2)}$
  \item $a^{(3)}=g(z^{(3)})$ (add $a^{(3)}_0$)
  \item $z^{(4)}=\theta^{(3)}\,a^{(3)}$
  \item $a^{(4)}=h_\theta(x)=g(z^{(4)})$
\end{itemize}

The gradient is computed using the backpropagation algorithm.
For each node $j$ in each leayer $l$ the ``error'' $\delta^{(l)}_j$ is determined
\begin{itemize}
  \item $\delta^{(4)}=a^{(4)}_j-y_j$
  \item $\delta^{(3)}=(\theta^{(3)})^\top\delta^{(4)}\odot g^\prime(z^{(3)})$
  \item $\delta^{(2)}=(\theta^{(2)})^\top\delta^{(3)}\odot g^\prime(z^{(2)})$
\end{itemize}
with $\odot$ denoting the elementwise multiplication and $g^\prime(z^{(l)})$ being the derivative of $g(z)=\frac{1}{1+\operatorname{e}^{-z}}$ at $z^{(l)}$.
\begin{equation*}
  g^\prime(z)=\frac{\operatorname{e}^{-z}}{(1+\operatorname{e}^{-z})^2}=g(z)\,\big(1-g(z)\big)
\end{equation*}
Note that $g(z^{(l)})=a^{(l)}$. \emph{I.e.}
\begin{equation*}
  g^\prime(z^{(l)})=a^{(l)}\,(1-a^{(l)})
\end{equation*}
There is no error associated with the input values. \emph{I.e.} $\delta^{(l)}$ is only defined for $l\in\{2,3,\ldots,L\}$.
Also the errors $\delta^{(l)}_0$ are discarded since the bias units need to be fixed to $1$.

It can be shown that the derivative of the cost function for $\lambda=0$ (no regularization) is
\begin{equation*}
  \frac{\delta}{\delta\theta^{(l)}_{ij}}J(\theta)=a^{(l)}_j\,\delta^{(l+1)}_i
\end{equation*}
For the complete training set the errors are accumulated for $i\in\{1,2,\ldots,m\}$ with $m$ being the number of training samples.
\begin{equation*}
  \Delta^{(l)}_{ij}\coloneqq\Delta^{(l)}_{ij}+a^{(l)}_j\,\delta^{(l+1)}_i
\end{equation*}
with $a^{(l)}_j$ and $\delta^{(l+1)}_i$ determined for each training sample $(\sxi,\syi)$.
Using vector notation the accumulation of errors also can be written as
\begin{equation*}
  \Delta^{(l)}\coloneqq\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^\top
\end{equation*}
The gradient of the cost function then is
\begin{equation*}
  \frac{\delta}{\delta\theta^{(l)}_{ij}}J(\theta)=D^{(l)}_{ij}\mathrm{\ where\ }
  D^{(l)}_{ij}\coloneqq\frac{1}{m}\Delta^{(l)}_{ij}+\left\{
    \begin{array}{ll}\lambda\theta^{(l)}_{ij}&\mathrm{if\ }j\neq 0\\
  0&\mathrm{if\ }j=0\end{array}\right.
\end{equation*}

See Appendix \ref{app:backprop} and \ref{app:mnistbackprop} for backpropagation example implementations.

\subsection{Gradient Checking}
To test correctness of an implementation of a backpropagation algorithm one can use numerical gradient checking.
Basically the gradient is numerically approximated using an infitesimal value $\epsilon$.
The symbolically derived gradient value must be close to the approximation as shown in the following equation
\begin{equation*}
\frac{\delta}{\delta\theta}J(\theta)\overset{!}{\approx}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\,\epsilon}
\end{equation*}
Concretely an ``unrolled'' version of $\theta=(\theta_1,\theta_2,\ldots,\theta_n)$ is used to check each partial derivative.
\begin{align*}
  \begin{split}
    \frac{\delta}{\delta\theta_1}J(\theta)&\overset{!}{\approx}\frac{J(\theta_1+\epsilon,\theta_2,\ldots,\theta_n)-J(\theta_1-\epsilon,\theta_2,\ldots,\theta_n)}{2\,\epsilon}\\
    \frac{\delta}{\delta\theta_2}J(\theta)&\overset{!}{\approx}\frac{J(\theta_1,\theta_2+\epsilon,\ldots,\theta_n)-J(\theta_1,\theta_2-\epsilon,\ldots,\theta_n)}{2\,\epsilon}\\
                                          &\vdots
  \end{split}
\end{align*}
Gradient checking is computationally expensive but it only is needed to verify the implementation when it has been changed.

\subsection{Random Initialization}
Linear regression can be initialised using $\theta=0$.
However when using hidden layers, the initial parameters must be distinct otherwise the activations will be equal to each other ($a^{(2)}_1=a^{(2)}_2=\ldots$).
Furthermore the errors will be equal to each other ($\delta^{(2)}_1=\delta^{(2)}_2=\ldots$) and
the partial derivatives of $J(\theta)$ will be equal to each other.
\emph{I.e.} even after one or more gradient descent updates, the parameters will be equal to each other.

To break the symmetry, one can use \emph{random initialization}.
Each parameter $\theta^{(l)}_{ij}$ is set to a random value in the interval $[-\epsilon,\epsilon]$ so that $\forall i,j,l:-\epsilon\le\theta^{(l)}_{ij}\le\epsilon$.

\subsection{Putting It Together}
Initially one needs to \textbf{pick a network architecture} (\emph{i.e.} number of layers $L$ and number of units $s_l$ in each layer.
The number of input units is determined by the dimension of the features $\sxi$ and the number of output units is determined by the number of classes.
The training labels $\syi$ are unit vectors with one unit vector for each class (see Section \ref{cha:multiclassif}).

A reasonable default is to use one hidden layer.
When using more than one hidden layer, a reasonable default is to use the same number of units in each hidden layer.
Usually the more hidden units the better (but also computationally more expensive).
Usually it is useful to have more hidden units in each layer than the number of input features.

The next step is to \textbf{train the neural network}
\begin{enumerate}
  \item Randomly initialize weights
  \item Implement forward propagation to get $h_\theta(\sxi)$ for any $\sxi$
  \item Implement code to cimpute cost function $J(\theta)$
  \item Implement backprop to compute partial derivatives $\frac{\delta}{\delta\theta^{(l)}_{jk}}J(\theta)$.
    \emph{I.e.} for $i\in\{1,2,\ldots,m\}$
    \begin{itemize}
      \item perform forward propagation and backpropagation using example $(\sxi,\syi)$
      \item Get activations $a^{(l)}$ and delta terms $\delta^{(l)}$ for $l\in\{2,\ldots,L\}$
      \item Accumulate delta term $\Delta^{(l)}\coloneqq\Delta^{(l)}+\delta^{(l+1)}\,(a^{(l)})^\top$
    \end{itemize}
  \item Use gradient checking to compare $\frac{\delta}{\delta\theta^{(l)}_{jk}}J(\theta)$ computed using backpropagation with the numerical estimate for the gradient of $J(\theta)$. Then disable gradient checking.
  \item Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\theta$.
\end{enumerate}

Since $J(\theta)$ is non-convex for neural networks, \emph{i.e.} gradient descent can get stuck in a local optimum.
However in practise this is usually not a problem (even if the global optimum is not found).

\subsection{Autonmous Driving Example}
The autonomous driving example shows a simple neural network trained using $30\times 30$ gray level images as input and
steering direction as output (see Figure \ref{fig:alvinn}).
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.6\textwidth]{alvinn}
    \caption{Autonomous driving using a neural network\citep{alvinn}\label{fig:alvinn}}
  \end{center}
\end{figure}

\section{Advice for Applying Machine Learning}
When machine learning does not yield good results, it is important to first establish what the problem is before
trying one of the following options
\begin{itemize}
  \item get more training examples
  \item try smaller set of features
  \item try getting additional features
  \item try adding polynomial features
  \item try decreasing $\lambda$
  \item try increasing $\lambda$
\end{itemize}

The data set is separated into a training set (60 \%), cross-validation set (20 \%), and test set (20 \%).
The sum-squared formula (without regularization term) is used to monitor training, to perform cross-validation, and to test the final result:
\begin{itemize}
  \item $J_{train}(\theta)=\frac{1}{2\,m}\,\sumi(h_\theta(\sxi)-\syi)^2$
  \item $J_{cv}(\theta)=\frac{1}{2\,m}\,\sumi(h_\theta(\sxi_{cv})-\syi_{cv})^2$
  \item $J_{test}(\theta)=\frac{1}{2\,m}\,\sumi(h_\theta(\sxi_{test})-\syi_{test})^2$
\end{itemize}
Note that the formulas do not include the regularization term.

Bias (underfit) and high variance (overfit) can be distinguised as follows
\begin{itemize}
  \item \textbf{underfit}: training and validation error both high
  \item \textbf{overfit}: training error low, validation error $>>$ training error
\end{itemize}

The validation data is to select between different models. It can also be used to choose an optimal regularization term. \emph{E.g.} compare $J_{cv}(\theta)$ for $\lambda\in\{0,0.01,0.02,\ldots,10.24\}$.

One can plot the \emph{learning curve} by showing $J_{train}$ and $J_{cv}$ for increasing training set size $m$.
The learning curve allow to detect \emph{bias} and \emph{high variance} more easily
\begin{itemize}
  \item \textbf{high variance}: model not fully converging (curves apart from each other) $\Rightarrow$ get more data or use simpler model
  \item \textbf{bias}: curves quickly converge at a high value and adding more training data does not improve results
\end{itemize}

In summary
\begin{itemize}
  \item get more training examples $\rightarrow$ fixes high variance
  \item try smaller set of features $\rightarrow$ fixes high variance
  \item try getting additional features $\rightarrow$ fixes high bias
  \item try adding polynomial features $\rightarrow$ fixes high bias
  \item try decreasing $\lambda$ $\rightarrow$ fixes high bias
  \item try increasing $\lambda$ $\rightarrow$ fixes high variance
\end{itemize}

When using neural networks, it is generally more effective to use a larger neural network (more hidden units and layers) and address overfitting using regularization (rather than working with a small neural network).
The remaining disadvantage is that larger neural networks are more computationally expensive.

It is important to \textbf{first implement a small prototype machine learning algorithm} to obtain early feedback from working with the data to inform further work on solving the machine learning problem.

\section{Machine Learning System Design}
\subsection{Prioritizing What to Work On}
Example: Spam email classifier with feature vector ``word $i$ occurs in email or not''.

How to spend your time to make it have low error?
\begin{itemize}
  \item collect lots of data (\emph{e.g.} ``honetpot'' project)
  \item develop sophisticated features based on email routing information (from email header)
  \item develop sophisticated features for message body (\emph{e.g.} should ``discount'' and ``discounts'' be treated as the same word)?
  \item detect deliberate misspellings (\emph{e.g.} m0rtgage, med1cine, w4tches)
\end{itemize}
Instead of randomly fixating on a task, one can use a more systematic way of choosing the next thing to work on.

\subsubsection{Recommended approach}
\begin{itemize}
  \item Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data.
  \item Plot learning curves to decide if more data, more features, etc. are likely to help.
  \item Error analysis: manually examine the examples (in cross validation set) that your algorithm made errors on.
    See if you spot any systematic trend in what type of examples it is making errors on.
\end{itemize}

It is important to use numerical evaluation, \emph{i.e.} one can quickly evaluate different techniques using the cross validation error. \emph{E.g.} use ``stemming'' or not, distinguish between upper and lower case or not.

In the case of the cancer classification example, only 0.5\% of patients have cancer.
\emph{I.e.} the classification error for the trivial classifier $h(x)=0$ already has 99.5\% accuracy.
In this case the \emph{precision/recall} metric is more appropriate.
\begin{center}
  \begin{tabular}{cc|cc}
    & & \multicolumn{2}{c}{actual class}\\
    &  & 1 & 0\\\hline
    \multirow{2}{*}{predicted class} & 1 & true positive & false positive\\
    & 0 & false negative & true negative\\
  \end{tabular}
\end{center}
Precision and recall are defined as follows:
\begin{itemize}
  \item $precision=\frac{true\ positives}{true\ positives+false\ positives}$ (ratio of true positives and predicted positives)
  \item $recall=\frac{true\ positives}{true\ positives+false\ negatives}$ (ratio of true positives and actual positives)
\end{itemize}

\subsection{Trading of Precision and Recall}\label{cha:f1}
Considering logistic regression $0\le h_\theta(x)\le 1$:
\begin{itemize}
  \item Suppose we want to predict $y=1$ (patient has cancer) only if very confident:
    Predict $1$ if $h_\theta(x)\ge 0.7$ (instead of $0.5$.
    In this case $\Rightarrow$ higher precision, lower recall.
  \item Suppose we want to avoid missing too many cases of cancer (avoid false negatives).
    In this case $\Rightarrow$ higher recall, lower precision.
\end{itemize}
More generally: Predict 1 if $h_\theta(x)\ge threshold$.

To make algorithms comparable, a single number metric, the $F_1$ score is defined:
\begin{equation*}
  F_1\ score=2\frac{P\,R}{P+R}
\end{equation*}

\subsection{Data For Machine Learning}
\citep{banko2001scaling} shows that data trumps algorithms:
``It’s not who has the best algorithms who wins, it’s who has the most data.''

Sometimes more features are required rather than more samples.

A useful test is: ``Given the input $x$, can a human expert confidently predict $y$?''

Using a large training set ensures low variance. \emph{I.e.} $J_{train}(\theta)\approx J_{test}(\theta)$.
This allows using a learning algorithm with many parameters so that $J_{test}(\theta)$ will be small.

\section{Support Vector Machines}
\subsection{Optimization Objective}
\acp{SVM} can be a powerful method for solving supervised learning problems.
Logistic regression uses
\begin{equation*}
  h_\theta(x)=\frac{1}{1+\operatorname{e}^{-\theta^\top x}}
\end{equation*}
\begin{itemize}
  \item if $y=1$, we want $h_\theta(x)\approx 1$, $\theta^\top x\gg0$
  \item if $y=0$, we want $h_\theta(x)\approx 0$, $\theta^\top x\ll0$
\end{itemize}
For $y=1$ the cost term is (also see Figure \ref{fig:logcost})
\begin{equation*}
  -\operatorname{log}\frac{1}{1+\operatorname{e}^{-\theta^\top x}}
\end{equation*}
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.4\textwidth]{logcost}
    \caption{Cost terms for $y=1$ and $y=1$ with $\theta=1$\label{fig:logcost}}
  \end{center}
\end{figure}

The cost function of logistic regression is
\begin{equation*}
  \mathop{\operatorname{argmin}}_\theta\frac{1}{m}\bigg[\sumi\syi\big(-\operatorname{log}h_\theta(\sxi)\big)+(1-\syi)\big(-\operatorname{log}(1-h_\theta(\sxi)\big)\bigg]+\frac{\lambda}{2\,m}\sum_{j=1}^n\theta_j^2
\end{equation*}
The \ac{SVM} uses a modified cost function
\begin{equation*}
  \mathop{\operatorname{argmin}}_\theta C\bigg[\sumi\syi cost_1(\theta^T\sxi)+(1-\syi)cost_0(\theta^\top\sxi)\bigg]+\frac{1}{2}\sum_{j=1}^n\theta_j^2
\end{equation*}
The parameter $C$ is used instead of $\frac{1}{\lambda}$ to control the regularization trade-off.

A \ac{SVM} does not output a probability. Instead it just makes a prediction:
\begin{equation*}
  h_\theta(x)=\left\{\begin{array}{rl}1&\mathrm{if }\theta^\top x\ge 0\\0&\mathrm{otherwise}\end{array}\right.
\end{equation*}
The constant offset $\theta_0$ is realised using $x_0=1$.

\subsection{Large Margin Intuition}
\acp{SVM} use the following non-differentiable terms for defining the cost function:
\begin{align*}
  \begin{split}
    cost_1(z)&=\operatorname{max}(1-z,0)\\
    cost_0(z)&=\operatorname{max}(z+1,0)
  \end{split}
\end{align*}
\begin{itemize}
  \item if $y=1$, we want $h_\theta(x)=1$, $\theta^\top x\ge 1$
  \item if $y=0$, we want $h_\theta(x)=0$, $\theta^\top x\le -1$
\end{itemize}
The \ac{SVM} will try to find a decision boundary which has a distance from the labelled data.
\ac{SVM} can give you a \emph{large margin classifier} which is more robust.

\subsection{Mathematics behind Large Margin Classification}
The regularization objective is to minimize
\begin{equation*}
  \frac{1}{2}\sum_{j=1}^n\theta_j^2
\end{equation*}
\emph{i.e.} to keep the norm of the parameter vector $||\theta||$ small.
Assuming the simplified case $\theta_0=0$, the objective of the cost function is to keep
\begin{align*}
  \begin{split}
    p^{(i)}\cdot||\theta||\ge 1&\mathrm{\ if\ }y^{(i)}=1\\
    p^{(i)}\cdot||\theta||\le -1&\mathrm{\ if\ }y^{(i)}=0
  \end{split}
\end{align*}
small, where $p^{(i)}$ is the projection of $\sxi$ onto the vector $\theta$.
\emph{I.e.} if the achievable margin is small, $||\theta||$ must be large.
If the margin is large, $||\theta||$ can be smaller.

When $\theta_0\ne 0$, there is a generalisation of this argument that shows, that the same holds for decision boundaries not passing through the origin.

\subsection{Kernels}
Non-linear decision boundaries can be achieved using higher-order polynomials (see Section \ref{cha:polyregression}).
\emph{E.g.} predict $y=1$ if
$\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\ldots\ge 0$:
\begin{equation*}
  h_\theta(x)=\left\{\begin{array}{rl}1&\mathrm{if\ }\theta_0+\theta_1 x_1+\ldots\ge 0\\0&\mathrm{otherwise}\end{array}\right.
\end{equation*}
With features $f_1=x_1$, $f_2=x_2$, $f_3=x_1 x_2$, $\ldots$.

A \ac{SVM} uses kernels to define new features to build sophisticated classifiers.
\emph{E.g.} one can define new features using the proximity to the landmarks $l^{(1)},l^{(2)},l^{(3)}$.
\begin{align*}
  \begin{split}
    f_1=similarity(x,l^{(1)})&=\operatorname{e}^{-\cfrac{||x-l^{(1)}||^2}{2\sigma^2}}=\operatorname{e}^{-\cfrac{\sum_{j=1}^n(x_j-l^{(1)}_j)^2}{2\sigma^2}}\\
    f_2=similarity(x,l^{(2)})&=\operatorname{e}^{-\cfrac{||x-l^{(2)}||^2}{2\sigma^2}}=\hspace{1cm}\vdots\\
    f_3=\underbrace{similarity}_{\mathrm{kernel}}(x,l^{(3)})&=\underbrace{\operatorname{e}^{-\cfrac{||x-l^{(3)}||^2}{2\sigma^2}}}_{\mathrm{Gaussian\ kernel}}=\hspace{1cm}\vdots
  \end{split}
\end{align*}
\emph{I.e.}
\begin{itemize}
  \item if $x\approx l^{(1)}$: $f_1\approx 1$
  \item if $x$ is far from $l^{(1)}$: $f_1\approx 0$
\end{itemize}
$\sigma$ is the size of the kernel's support region.

In practice all training examples are chosen as landmarks, so that there are $m$ landmarks (where $m$ is the size of the training set):\\
Given $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})$\\
choose $l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},\ldots l^{(m)}=x^{(m)}$.\\
For an example $x$, the features values are
\begin{align*}
  \begin{split}
    f_1&=similarity(x,l^{(1)})\\
    f_2&=similarity(x,l^{(2)})\\
    &\vdots
  \end{split}
\end{align*}
For the training example $(\sxi,\syi)$ the feature values are
\begin{align*}
  \begin{split}
    f^{(i)}_1&=similarity(x^{(1)},l^{(1)})\\
    f^{(i)}_2&=similarity(x^{(2)},l^{(2)})\\
    &\vdots
  \end{split}
\end{align*}
For each $\sxi\in\mathbb{R}^{n+1}$ we obtain a feature vector
\begin{equation*}
  f^{(i)}=\begin{pmatrix}f^{(i)}_0\\f^{(i)}_1\\f^{(i)}_2\\\vdots\\f^{(i)}_m\end{pmatrix}
    \mathrm{\ with\ }f^{(i)}_0=1
\end{equation*}

Given $x$, the \ac{SVM} computes the features $f\in\mathbb{R}^{m+1}$ and makes a prediction using the parameter vector $\theta\in\mathbb{R}^{m+1}$:
\begin{itemize}
  \item predict ``y=1'' if $\theta^\top\,f\ge 0$
  \item predict ``y=0'' otherwise
\end{itemize}
\newcommand{\sfi}{\ensuremath{f^{(i)}}}
\newcommand{\sli}{\ensuremath{l^{(i)}}}
An \ac{SVM} is trained using the cost function based on the feature vectors
\begin{equation*}
  \mathop{\operatorname{argmin}}_\theta C\bigg[\sumi\syi cost_1(\theta^T\sfi)+(1-\syi)cost_0(\theta^\top\sfi)\bigg]+\frac{1}{2}\sum_{j=1}^m\theta_j^2
\end{equation*}
Note that $n=m$ and we still do not regularize $\theta_0$.

The parameter $C$($=\frac{1}{\lambda}$) controls the bias and variance trade-off:
\begin{itemize}
  \item large $C$: lower bias, high variance
  \item small $C$: higher bias, low variance
\end{itemize}
The parameter $\sigma$ controls the smoothness
\begin{itemize}
  \item large $\sigma^2$: features $f_i$ vary more smoothly $\rightarrow$ higher bias, lower variance
  \item small $\sigma^2$: features $f_i$ vary less smoothly $\rightarrow$ lower bias, higher variance
\end{itemize}

\subsection{Using an SVM}
It is recommended to use an existing highly optimized \ac{SVM} libraries (\emph{e.g.}
liblinear\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/liblinear/}},
libsvm\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}}, ...) to solve for parameters $\theta$.
However the software should let you choose the parameter $C$ and the kernel (similarity function).\\
\emph{E.g.}
\begin{itemize}
  \item no kernel (``linear kernel''): Predict ``y=1'' if $\theta^\top x\ge 0$
  \item Gaussian kernel (when $n\ll m$): $f_i=\operatorname{e}^{-\cfrac{||x-\sli||^2}{2\sigma^2}}$, where $\sli=\sxi$, need to choose $\sigma^2$.
  \item Polynomial kernels: $k(x,l)=(x^\top l+constant)^{degree}$
  \item More esoteric: string kernel, chi-square kernel, histogram intersection kernel, ...
\end{itemize}
Note: do perform feature scaling before using the Gaussian kernel.

Not all similarity functions $similarity(x,l)$ make valid kernels.
An \ac{SVM} kernel needs to satisfy ``Mercer's Theorem'' to make sure \ac{SVM} packages' optimizations run correctly, and do not diverge.

Many \ac{SVM} packages already have built-in multi-class classification functionality.
Otherwise, one can use the one-vs-all method: train $K$ \acp{SVM} to distinguish $y=i$ from the rest for $i\in\{1,2,\ldots,K\}$ resulting in parameter vectors $\theta^{(1)},\theta^{(2)},\ldots,\theta^{(K)}$ and pick class $i$ with largest $(\theta^{(i)})^\top f$.

When $n$ is the number of features (\emph{e.g.} $x\in\mathbb{R}^{n+1}$ and
$m$ is the number of training examples:
\begin{itemize}
  \item if $n$ is large (relative to $m$): use logistic regression, or \ac{SVM} without a kernel (``linar kernel'')
  \item if $n$ is small, $m$ is intermediate: use \ac{SVM} with Gaussian kernel
  \item if $n$ is small, $m$ is large: create/add more features, then use logistic regression or \ac{SVM} without a kernel
\end{itemize}
Neural network likely to work well for most of these settings, but may be slower to train.

\section{Clustering}
\subsection{Unsupervised Learning}
Supervised learning uses a labelled training set
$\{x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}),\ldots,(x^{(m)},y^{(m)})\}$
and tries to fit a hypothesis to it.
In contrast the unsupervised learning problem is to find some structure in an unlabelled training set
$\{x^{(1)},x^{(2)},x^{(3)},\ldots,x^{(m)}\}$.

Clustering algorithms can find clusters in unlabelled data. Examples are:
\begin{itemize}
  \item market segmentation
  \item social network analysis
  \item organizing computing clusters
  \item astronomical data analysis
\end{itemize}

\subsection{K-Means Algorithm}
The input is
\begin{itemize}
  \item $K$ (number of cluster)
  \item training set $\{x^{(1)},x^{(2)},x^{(3)},\ldots,x^{(m)}\}$, $\sxi\in\mathbb{R}^n$
\end{itemize}
The \emph{K-means} algorithm randomly selects $k$ cluster centroids
$\mu_1,\mu_2,\ldots,\mu_K\in\mathbb{R}^n$.
Each iteration then
\begin{enumerate}
  \item \emph{cluster assignment step}: assigns each data point $\sxi$ to the nearest cluster centroid $\mu_j$ (\emph{i.e.} $c^{(i)}\coloneqq\displaystyle\mathop{\operatorname{argmin}}_j ||\sxi-\mu_j||$)
  \item \emph{move centroids step}: the centroids are set to the mean location of each cluster $\mu_k\coloneqq\displaystyle\sum_{i,c^{(i)}=k}\sxi\bigg/\sum_{i,c^{(i)}=k}1$
\end{enumerate}
Note that there is no $x_0=1$ convention here.
If a cluster is empty, the cluster centroid needs to be eliminated or initalised with a new random position.

\subsection{Optimization Objective}
The optimization objective of the K-means algorithm is
\begin{equation*}
  \mathop{\operatorname{argmin}}_{\substack{c^{(1)},\ldots,c^{(m)}\\\mu_1,\ldots,\mu_K}}J(c^{(1)},\ldots,c^{(m)},\mu_1,\ldots,\mu_K)
\end{equation*}
using
\begin{equation}\label{equ:kmeanscost}
  J(c^{(1)},\ldots,c^{(m)},\mu_1,\ldots,\mu_K)=\frac{1}{m}\sumi||\sxi-\mu_{c^{(i)}}||
\end{equation}

\subsection{Random Initialization}
The random initialization of the K-means algorithm is recommended to be performed as follows.
\begin{itemize}
  \item $K\overset{!}{<}m$
  \item randomly pick $K$ training examples
  \item set the initial centroids $\mu_1,\ldots,\mu_K$ equal to these $K$ examples
\end{itemize}

K-means can get stuck in a local optimum (especially when $K$ is small). One can address this by running the K-means algorithm many times and chosing the best result using the cost function $J$ (see Equation (\ref{equ:kmeanscost})).

\subsection{Choosing the Number of Clusters}
The elbow method for choosing $K$ works by plotting the optimal cost functions value $J$ for different $K$.
The value where $J$ stops going down rapidly is chosen as the optimal $K$ value.
Often the elbow method does not give a clear answer though.

Sometimes the value $K$ can be chosen by considering the later/downstream purpose.
K-means then is evaluated based on a metric for how well it performs for that later purpose
(\emph{e.g.} manufacture 3 or 5 different t-shirt sizes for customers, image compression, ...).

Other method for clustering: DBSCAN.

\section{Dimensionality Reduction}
\subsection{Motivation 1: Data Compression}
For example one feature could be length in centimeter (rounded) and the other length in inches (rounded).
This 2D data $\sxi\in\mathbb{R}^2$ ($i\in\{1,2,\ldots,m\}$) can be reduced/projected to 1D $z^{(i)}\in\mathbb{R}$.
Less data means less storage space required and more importantly less processing time required for training machine learning algorithms.

Another example is reduction from 3D to 2D (in general dimensionality reduction applies to arbitrary numbers of dimensions).
In this case 3D features are projected onto a plane so that $z^{(i)}\in\mathbb{R}^2$.

\subsection{Motivation 2: Data Visualization}
Data visualization is important when trying to decide how to improve a machine learning algorithm.
Dimensionality reduction can make it possible to visualize data with a high number of dimensions.

\subsection{Principal Component Analysis}
The most commonly used algorithm for dimensionality reduction is \ac{PCA}.
First feature scaling needs to be applied so that the features are zero-mean and have similar standard deviations.
\begin{itemize}
  \item reduce from 2-dimension to 1-dimension: find a vector $u^{(1)}\in\mathbb{R}^n$ onto which to project the data as to minimize the projection error
  \item reduce from n-dimension to k-dimension: find $k$ vectors $u^{(1)},u^{(2)},\ldots,u^{(k)}$ onto which to project the data as to minimize the projection error
\end{itemize}
Note that \ac{PCA} is not the same as linear regression. Linear regression minimises the \emph{vertical} distance, while PCA minimises the projected distance.

\subsection{Principal Component Analysis Algorithm}\label{cha:pca}
Given: a zero-mean set of features with a normalised standard deviation $\{x^{(1)},x^{(2)},\ldots,x^{(n)}\}$.\\
Desired: a set of features $\{z^{(1)},z^{(2)},\ldots,z^{(n)}\}$ with a lower number of dimensions.\\
\begin{itemize}
  \item compute ``covariance matrix'' $\Sigma=\cfrac{1}{m}\sum_i^n(\sxi)(\sxi)^\top$
  \item compute ``eigenvectors'' of $\Sigma$ (or \ac{SVD}) so that $\Sigma=\mathcal{U}\mathcal{S}\mathcal{V}^\top$
\end{itemize}
$\mathcal{U}$ contains the vectors to project the features onto in descending order of importance.
\begin{equation*}
  \mathcal{U}=\begin{pmatrix}u^{(1)}&u^{(2)}&\cdots&u^{(n)}\end{pmatrix}
\end{equation*}
The first $k$ columns of $\mathcal{U}$ are used to obtain the projected features $z^{(i)}$.
\begin{equation*}
  z=\underbrace{\begin{pmatrix}u^{(1)}&u^{(2)}&\cdots&u^{(k)}\end{pmatrix}^\top}_{\eqqcolon\mathcal{U}_{reduce}^\top} x
\end{equation*}
Note that here the convention $x_0=1$ is not used!

\subsection{Choosing the Number of Principal Components}
The number $k$ (number of \acp{PCA} to retain) results in a certain projection error
\begin{equation*}
  \frac{1}{m}\sumi||\sxi-x^{(i)}_{approx}||^2
\end{equation*}
The total variation in the data is
\begin{equation*}
  \frac{1}{m}\sumi||\sxi||^2
\end{equation*}
$k$ is chosen, so that the projection error is small relative to the total variation in the data. \emph{E.g.}
\begin{equation*}
  \frac{\sumi||\sxi-x^{(i)}_{approx}||^2}{\sumi||\sxi||^2}\le 0.01
\end{equation*}
in this case ``99\% of variance is retained''
Using the matrix of singular values $\mathcal{S}$ of the covariance matrix (see Section \ref{cha:pca})
\begin{equation*}
  \mathcal{S}=\operatorname{diag}(s_{11}\,s_{22}\,\ldots\,s_{nn})
\end{equation*}
one can limit the loss in variance as follows
\begin{equation*}
  1-\frac{\sum_{i=1}^k s_{ii}}{\sum_{i=1}^n s_{ii}}\le 0.01
\end{equation*}

For visualisation usually $k=2$ or $k=3$ is used.

\subsection{Reconstruction from Compressed Representation}
The projected sample $z^{(i)}$ can be projected back to get the uncompressed representation which is an approximation of the initial feature set. This process is called ``reconstruction''.
\begin{equation*}
  \underbrace{\sxi_{approx}}_{\in\mathbb{R}^n}=\underbrace{\mathcal{U}_{reduce}}_{\in\mathbb{R}^{n\times k}}\cdot\underbrace{z^{(i)}}_{\in\mathbb{R}^k}
\end{equation*}

\subsection{Advice for Applying PCA}
Given a large dataset $x^{(i)}$, one can use an unlabeled dataset and apply \ac{PCA} to it, in order to obtain a lower-dimensional representation $z^{(i)}$ in order to
\begin{itemize}
  \item speed up the learning algorithm and
  \item reduce memory/disk needed to store the data
\end{itemize}
Note that the input for the predictor needs to be mapped the same way using $\mathcal{U}_{reduce}$ obtained from the training set. \emph{I.e.} \ac{PCA} is only performed on the training set and not on the cross validation $x^{(i)}_{cv}$ and test set $x^{(i)}_{test}$.

\ac{PCA} should not be used to prevent overfitting since \ac{PCA} could throw away valuable information.
The appropriate method to prevent overfitting is regularization (see Section \ref{cha:regularization}).

Before you implement \ac{PCA}, consider working with the original/raw data first.

\section{Anomaly Detection Problem}
\subsection{Motivation}
Given a dataset $\{x^{(1)},x^{(2)},\ldots,x^{(m)}\}$ is $x_{test}$ anomalous?

The model is a probability distribution $p(x)$. A test sample is considered an anomaly if the probability is below a certain threshold $\epsilon$:
\begin{equation*}
  p(x_{test})<\epsilon:\Leftrightarrow x_{test}\mathrm{\ is\ an\ anomaly}
\end{equation*}

Example applications are
\begin{itemize}
  \item fraud detection (monitoring online behaviour of users)
  \item manufacturing (detecting faulty parts)
  \item monitoring computers in a data center
\end{itemize}

\subsection{Gaussian Distribution}
Say $x\in\mathbb{R}$ is a variable with Gaussian distribution with mean $\mu$ and variance $\sigma^2$
\begin{equation*}
x\in\mathbb{R}\sim\mathcal{N}(\mu,\sigma^2)
\end{equation*}
The symbol $\sim$ means ``distributed as''. The probability distribution is
\begin{equation*}
  p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\operatorname{e}^{-\cfrac{(x-\mu)^2}{2\sigma^2}}
\end{equation*}

The parameters of the distribution can be estimated as follows
\begin{itemize}
  \item $\mu=\frac{1}{m}\sumi\sxi$
  \item $\sigma^2=\frac{1}{m}\sumi(\sxi-\mu)^2$
\end{itemize}
These parameters are actually the maximum likelihood estimates of the parameters.

\subsection{Anomaly Detection Algorithm}\label{cha:anomaly}
Given a training set $\{x^{(1)},x^{(2)},\ldots,x^{(m)}\}$ with each $x\in\mathbb{R}^n$
we assume independent Gaussian distributions
\begin{itemize}
\item $x_1\sim\mathcal{N}(\mu_1,\sigma^2_1)$
\item $x_2\sim\mathcal{N}(\mu_2,\sigma^2_2)$
\item $x_3\sim\mathcal{N}(\mu_3,\sigma^2_3)$
\item $\ldots$
\end{itemize}
The overall probability $p(x)$ is the product of the individual probabilities
\begin{equation*}
  p(x)=p(x_1;\mu_1,\sigma_1^2) p(x_2;\mu_2,\sigma_2^2) p(x_3;\mu_3,\sigma_3^2) \ldots=
  \prod_{j=1}^n p(x_j;\mu_j,\sigma_j^2)
\end{equation*}
The anomaly detection algorithm works as follows
\begin{enumerate}
  \item Choose features $x_i$ that you think are indicative of anomalous examples
  \item Fit parameters $\mu_1,\ldots,\mu_n,\sigma_1^2,\ldots,\sigma_n^2$
    \begin{itemize}
    \item $\mu_j=\frac{1}{m}\sumi\sxi_j$
    \item $\sigma_j^2=\frac{1}{m}\sumi(\sxi_j-\mu_j)^2$
    \end{itemize}
  \item Given a new example $x$, compute $p(x)=\prod_{j=1}^n p(x_j;\mu_j,\sigma_j^2)$. The example is considered an anomaly if $p(x)<\epsilon$.
\end{enumerate}

\subsection{Developing and Evaluating an Anomaly Detection System}
\begin{itemize}
  \item Use unlabelled training data (ideally not containing any anomalies).
  \item Use cross-validation and test-data containing a small number of known anomalies.
\end{itemize}
\emph{E.g.} given a data set with 10000 good (normal) engines and 20 flawed engines.
Divide the data into
\begin{itemize}
  \item training set: 6000 good engines
  \item cross-validation set: 2000 good engines (y=0), 10 anomalous (y=1)
  \item test: 2000 good engines (y=0), 10 anomalous (y=1)
\end{itemize}
\begin{equation*}
  y=\left\{\begin{array}{ll}1&\mathrm{if\ }p(x)<\epsilon\\0&\mathrm{if\ }p(x)\ge\epsilon\end{array}\right.
\end{equation*}
Possible evaluation metrics are
\begin{itemize}
  \item true positive, false positive, true negative, false negative
  \item precision/recall
  \item $F_1$-score
\end{itemize}
Cross-validation can also be used to choose $\epsilon$.
Note that the data is highly skewed (y=1 is rate), it is necessary to use a good evaluation metric (see Section\ref{cha:f1}).

\subsection{Anomaly Detection vs. Supervised Learning}
\begin{minipage}[t]{.45\textwidth}
  \center{\textbf{Anomaly detection}}
  \begin{itemize}
    \item Very small number of positive examples (y=1). 0-20 is common (\emph{e.g.} manufacturing)
    \item Large number of negative (y=0) examples
    \item Many different ``types'' of anomalies.
      Hard for any algorithm to learn from positive examples what the anomalies look like.
    \item Future anomalies may look nothing like any of the anomalous examples we've seen so far.
  \end{itemize}
\end{minipage}
\begin{minipage}[t]{.45\textwidth}
  \center{\textbf{Supervised learning}}
  \begin{itemize}
    \item Large number of positive and negative example (\emph{e.g. spam email classification})
    \item Enough positive exmaples for algorithm to get a sense of what positive examples are like,
      future positive examples likely to be similar to ones in training set.
  \end{itemize}
\end{minipage}

\subsection{Choosing What Features to Use}
One can plot a histogram to check whether the distribution is Gaussian. If the distribution is not Gaussian, one can use a transform (\emph{e.g.} log-transform) to obtain a feature with a Gaussian distribution.

\subsubsection{Error analysis for anomaly detection}
Desired are features so that $p(x)$ large for normal examples $x$ and small for anomalous examples.
The most common problem is that $p(x)$ is comparable for normal and anomalous examples.
In this case additional feature(s) is/are needed to distinguish the anomaly from the other normal examples.
\emph{E.g.} the ratio of two existing features to identify unusual combination of two features.

\subsection{Multivariate Gaussian Distribution}
One can use a multivariate Gaussian distribution to detect unusual combinations of features.
In this case the joint distribution $p(x)$ is modelled instead of modelling $p(x_1),p(x_2),\ldots$ separately.
\begin{equation*}
  p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^\frac{1}{2}}\operatorname{e}^{-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)}
\end{equation*}
where $|\Sigma|$ is the determinant of $\Sigma$.

\subsection{Anomaly Detection using the Multivariate Gaussian Distribution}
Similar as in Section \ref{cha:anomaly} the anomaly detection using the multivariate Gaussian distribution is implemented as follows
\begin{enumerate}
  \item Fit a model $p(x)$ to the training data
  \begin{itemize}
    \item $\mu_j=\frac{1}{m}\sumi\sxi_j$
    \item $\Sigma=\frac{1}{m}\sumi(\sxi-\mu)(\sxi-\mu)^\top$
  \end{itemize}
  \item Given a new example compute
    $p(x;\mu,\Sigma)=\displaystyle\frac{1}{(2\pi)^{n/2}|\Sigma|^\frac{1}{2}}\operatorname{e}^{-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)}$
\end{enumerate}
Flag $x$ as anomaly if $p(x)<\epsilon$.

The multivariate Gaussian approach automatically captures correlations between features.
However it is computationally more expensive if $n$ is large.
Also the size of the training set $m$ must be larger than the number of features $n$ (otherwise $\Sigma$ is not invertible).
In practise $m$ should be an order of magnitude larger than $n$ (rule of thumb $m\ge 10\,n$).

Note that the features need to be non-redundant otherwise the features might be linear-dependent so that $\Sigma$ is non-invertible.

\section{Recommender Systems}
\subsection{Content Based Recommendations}
Recommender systems are a popular application of machine learning (\emph{e.g.} Amazon product recommendation).

For example one can attempt to predict movie ratings for new movies not rated by a user jet.
The problem is formulated similar to the linear regression problem.
\begin{itemize}
  \item $r(i,j)=1$ if user $j$ has rated movie $i$ ($0$ otherwise)
  \item $y^{(i,j)}$ is the rating by user $j$ on movie $i$ (if defined)
  \item $\theta^{(j)}$ parameter vector for user $j$
  \item $\sxi$ feature vector for movie $i$ (\emph{e.g.} how much romance, how much action, ...)
  \item $m^{(j)}$ number of movies rated by user $j$
\end{itemize}
The predicted rating for user $j$ and movie $i$ is $(\theta^{(j)})^\top\sxi$.
$\theta^{(j)}$ is learned using
\begin{equation*}
  \mathop{\operatorname{argmin}}_{\theta^{(j)}}\frac{1}{2\,\cancel{m^{(j)}}}\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)^2+\frac{\lambda}{2\,\cancel{m^{(j)}}}\sum_{k=1}^n(\theta^{(j)}_k)^2
\end{equation*}
The optimization objective for all users $\theta^{(1)},\theta^{(2)},\ldots,\theta^{(n_u)}$ is:
\begin{equation*}
  \mathop{\operatorname{argmin}}_{\theta^{(1)},\ldots,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta^{(j)}_k)^2
\end{equation*}
The gradient descent update rule is
\begin{align*}
  \begin{split}
    \theta_k^{(j)}&\coloneqq\theta_k^{(j)}-\alpha\,\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)\sxi_k\ (\mathrm{for\ }k=0)\\
    \theta_k^{(j)}&\coloneqq\theta_k^{(j)}-\alpha\,\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)\sxi_k+\lambda\theta^{(j)}_k\ (\mathrm{for\ }k\ne 0)
  \end{split}
\end{align*}
Note that the content based approach requires feature vectors $\theta^{(j)}$.

\subsection{Collaborative Filtering}
In this example each user provides a preference vector $\theta^{(j)}$ for different types of movies.
It turns out that given $\theta^{(1)},\ldots,\theta^{(n_u)}$ one can infer the feature vectors $\sxi$ for a movie:
\begin{equation*}
  \mathop{\operatorname{argmin}}_{\sxi}\frac{1}{2}\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)^2+\frac{\lambda}{2}\sum_{k=1}^n(\sxi_k)^2
\end{equation*}
or for all the movies:
\begin{equation*}
  \mathop{\operatorname{argmin}}_{x^{(1)},\ldots,x^{(n_m)}}\frac{1}{2}\sum_{i=1}^{n_m}\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(\sxi_k)^2
\end{equation*}

So far it was shown that
\begin{itemize}
  \item Given $x^{(1)},\ldots,x^{(n_m)}$ one can estimate $\theta^{(1)},\ldots,\theta^{(n_u)}$
  \item Given $\theta^{(1)},\ldots,\theta^{(n_u)}$ one can estimate $x^{(1)},\ldots,x^{(n_m)}$
\end{itemize}
One can use this two algorithms starting with a random guess for $\theta$ to estimate $x$ and then use $x$ to get an improved estimate for $\theta$ ... going back and forth.

\subsection{Collaborative Filtering Algorithm}
A more efficient approach is to jointly optimize $x^{(1)},\ldots,x^{(n_m)}$ and $\theta^{(1)},\ldots,\theta^{(n_u)}$.
\emph{I.e.} solving
\begin{equation*}
  \mathop{\operatorname{argmin}}_{\substack{x^{(1)},\ldots,x^{(n_m)}\\\theta^{(1)},\ldots,\theta^{(n_u)}}}
  J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)})
\end{equation*}
where
\begin{equation}\label{equ:collaborative}
  J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)})=
  \frac{1}{2}\sum_{(i,j);r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)^2
  +\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(\sxi_k)^2
  +\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta^{(j)}_k)^2
\end{equation}
Note that it is not necessary to hardcode a constant feature $\theta_0=1$ or $x_0=1$ because the algorithm can learn those features if required.

The collaborate filtering algorithm works as follows
\begin{enumerate}
  \item Initialize $x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)}$ to small random values
  \item Minimize $J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)})$ using gradient descent
    \begin{itemize}
      \item $\sxi_k\coloneqq\sxi_k-\alpha\,\sum_{j;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)\theta^{(j)}_k+\lambda\sxi_k$
      \item $\theta_k^{(j)}\coloneqq\theta_k^{(j)}-\alpha\,\sum_{i;r(i,j)=1}\big((\theta^{(j)})^\top\sxi-y^{(i,j)}\big)\sxi_k+\lambda\theta^{(j)}_k$
    \end{itemize}
  \item For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^\top x$
\end{enumerate}

\subsection{Vectorization: Low Rank Matrix Factorization}
Equation (\ref{equ:collaborative}) can be vectorized using
\begin{itemize}
  \item a matrix $\mathcal{Y}$ composed from $y^{(i,j)}$, note that some values will be undefined
  \item a matrix $\mathcal{X}=\begin{pmatrix}(x^{(1)})^\top\\(x^{(2)})^\top\\\hdots\\(x^{(n_m)})^\top\end{pmatrix}$ of the movie feature vectors
  \item a matrix $\Theta=\begin{pmatrix}\theta^{(1)}&\theta^{(2)}&\hdots&\theta^{(n_u)}\end{pmatrix}$ of the user parameters
\end{itemize}
$\mathcal{X}$ and $\Theta$ are assumed to be low rank matrices (\emph{i.e.} movies and users can be clustered).
The objective then becomes to minimize $\mathcal{X}\Theta-Y$ (where $\mathcal{Y}$ is defined).

The feature vectors $\sxi$ for each movie offer a convenient way to find similar movies:
\begin{equation*}
  ||x^{(i)}-x^{(j)}||\mathrm{\ is\ small}\Leftrightarrow\mathrm{movies\ }i\mathrm{\ and\ }j\mathrm{\ are\ ``similar''}
\end{equation*}

\subsection{Implementation Detail: Mean Normalization}
If a user $\theta^{(j)}$ has not rated any movies, then the regularization term will cause $\theta^{(j)}=0$ to be the optimal solution. \emph{I.e.} there is no recommendation available.

Mean normalization uses a zero-mean version of $\mathcal{Y}$.
The prediction of a movie rating is $(\theta^{(j)})^\top x^{(i)}+\mu_i$ where $\mu$ is the average feature vector used to obtain the zero-mean rating matrix $\mathcal{Y}$.
\emph{I.e.} if a user has not provided any ratings, the predicted movie rating is the average rating.

Note that it is more important to predict a rating for a user which has not provided any ratings, than to predict a rating for a movie which has not received any ratings.

\section{Large Scale Machine Learning}
\subsection{Learning with Large Datasets}
If $m$ is large (\emph{e.g.} $m=100,000,000$), the computational expense of a gradient descent step such as
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\sxi_j
\end{equation*}
is very high.
If the model is very simple (training and validation cost function converge quickly), a small random subset of the data might be sufficient to train the learning algorithm.

\subsection{Stochastic Gradient Descent}
Without loss of generality, suppose we are performing linear regression (see Section \ref{cha:linearmulti}) for
\begin{itemize}
  \item $h_\theta(x)=\displaystyle\sum_{j=0}^n\theta_jx_j$
  \item $J_{train}(\theta)=\cfrac{1}{2\,m}\sumi\big(h_\theta(\sxi)-\syi\big)^2=\frac{1}{m}\sumi cost\big(\theta,(\sxi,\syi)\big)$
\end{itemize}
using gradient descent
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\underbrace{\frac{1}{m}\sumi\big(h_\theta(\sxi)-\syi\big)\sxi_j}_{=\frac{\delta}{\delta\theta}J_{train}(\theta)}
  \mathrm{\ for\ }j\in\{0,\ldots,n\}
\end{equation*}
Summing over a large $m$ is computationally expensive.
Instead one can use \ac{SGD} (also known as batch gradient descent).
\emph{E.g.} using a single sample for a gradient descent step:
\begin{enumerate}
  \item Randomly shuffle dataset
  \item For $i\in\{1,\ldots,m\}$
    \begin{enumerate}
      \item $\theta_j\coloneqq\theta_j-\alpha\big(h_\theta(\sxi)-\syi\big)\sxi_j$ for $j\in\{0,\ldots,n\}$
    \end{enumerate}
\end{enumerate}
\ac{SGD} generally requires much less performance than a gradient descent using the full training set at each step.

\subsubsection{Mini-batch Gradient Descent}
There are three variations of the gradient descent algorithm:
\begin{itemize}
  \item Batch gradient descent: Use all $m$ examples in each iteration
  \item \acf{SGD}: Use 1 example in each iteration
  \item Mini-batch gradient descent: Use $b$ examples in each iteration (typically $2\le b\le100$)
\end{itemize}
The gradient update rule in this case is
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\frac{1}{b}\sum_{k=i}^{i+b-1}\big(h_\theta(x^{(k)})-y^{(k)}\big)x^{(k)}_j
  \mathrm{\ using\ }i=1,b+1,2\,b+1,\ldots
\end{equation*}
Mini-batch gradient descent can be parallelized more easily to gain computational speed.

\subsection{Stochastic Gradient Descent Convergence}
Checking for convergence can also be costly when $m$ is large.
Instead of computing the value of $J_{train}$ when doing \ac{SGD}, one can compute $cost(\theta,(\sxi,\syi))$ for the training sample under consideration before updating $\theta$ and every 1000 iterations plot the average cost of the last 1000 examples processed by the algorithm.
\begin{itemize}
  \item it might be necessary to average over more samples if the curve is too noisy (disadvantage is delayed feedback)
  \item Sometimes a smaller learning rate $\alpha$ can lead to better results especially if the cost is diverging
\end{itemize}
One can slowly decrease the learning rate $\alpha$ with the number of iterations. However this adds an additional parameter to tune.

\subsection{Online Learning}
An online learning algorithm can work with a continuous stream of data.
\emph{E.g.} on a shipping service website users specify origin and destination of shipment and after receiving a quote for the price, they either decide to use the service (y=1) or not (y=0). We want to learn $p(y=1|x;\theta)$ to optimize the price (the price is part of $x$) using logistic regression.
\begin{itemize}
  \item Get $(x,y)$ corresponding to a user
  \item Update $\theta$ using $(x,y)$: $\theta_j\coloneqq\theta_j-\alpha\,\big(h_\theta(x)-y)x_j$ for $j\in\{0\,\ldots,n\}$
\end{itemize}
This online learning algorithm will adapt to changing user preferences.
Note that if there is enough incoming data, it might be sufficient to look at each training sample once.

Another example would be the problem of which product results to display when a user enters a certain search term.
\begin{itemize}
  \item $x$ is the feature vector of the phone (how many words in user query match the name of the phone, how many words match the description of the phone)
  \item $y=1$ if user clicks on the link, $y=0$ otherwise.
  \item learn $p(y=1|x;\theta)$
\end{itemize}
This problem is known as the problem of predicting \ac{CTR}.

Other examples are: Choosing special offsers to show sers; customized selection of news articles; product recommendations; ...

\subsection{MapReduce and Data Parallelism}
For large scale machine learning problems it might be necessary to distribute the computation over several machines (see Hadoop\footnote{\url{http://hadoop.apache.org/}}).

\subsubsection{MapReduce}
\citep{mapreduce} introduces a simple approach to parallel computing on a large scale.
The gradient descent algorithm
\begin{equation*}
  \theta_j\coloneqq\theta_j-\alpha\frac{1}{m}\big(h_\theta(\sxi)-\syi)\sxi_j
\end{equation*}
can be parallelized using the MapReduce approach as follows
\begin{enumerate}
  \item Map step:
    \begin{itemize}
      \item machine 1
        \begin{itemize}
          \item use $(x^{(1)},y^{(1)}),\ldots,(x^{(100)},y^{(100)})$
          \item compute $temp^{(1)}_j=\sum_{i=1}^{100}\big(h_\theta(\sxi)-\syi)x^{(i)}_j$
        \end{itemize}
      \item machine 2
        \begin{itemize}
          \item use $(x^{(101)},y^{(101)}),\ldots,(x^{(200)},y^{(200)})$
          \item compute $temp^{(2)}_j=\sum_{i=101}^{200}\big(h_\theta(\sxi)-\syi)x^{(i)}_j$
        \end{itemize}
      \item $\vdots$
    \end{itemize}
  \item Reduce step: $\theta_j\coloneqq\theta_j-\alpha\frac{1}{m}(temp^{(1)}_j+temp^{(2)}_j+\ldots)$ for $j\in\{0,\ldots,n\}$
\end{enumerate}
Logistic regression can be parallelized in a similar fashion.

MapReduce can also be applied when using multi-core, vector instructions, or \acp{GPU}.
The Tensorflow\footnote{\url{https://www.tensorflow.org/}} implementation supports \acp{GPU} and \ac{SIMD}.

\section{Further Reading}
\begin{itemize}
  \item \acp{CNN}
  \item \ac{LSTM}
  \item \ac{RBM}
\end{itemize}

\clearpage

\appendix
\section{Appendix}
\subsection{Least Squares Estimator}\label{app:lse}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{least_squares.py}

\subsection{Linear Regression}\label{app:gradientdescent}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{gradient_descent.py}

\subsection{Learning Rate}\label{app:alphas}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{learning_rate.py}

\subsection{Logistic Regression}\label{app:classifier}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{classifier.py}

\subsection{Polynomial Features}\label{app:polynomial}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{polynomial.py}

\subsection{Overfitting Example}\label{app:overfitting}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{overfitting.py}

\subsection{Backpropagation using Tensorflow}\label{app:backprop}
Backpropagation example from \url{https://stackoverflow.com/questions/44210561/how-do-backpropagation-works-in-tensorflow}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{backprop.py}

\subsection{Backpropagation applied to the MNIST Dataset}\label{app:mnistbackprop}
\subsubsection{Training}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{mnist-backprop.py}
\subsubsection{Testing}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{mnist-run.py}

\subsection{Convolution using Theano}
\inputminted[frame=lines,linenos,fontsize=\small]{python}{convolution.py}

\section{References and Glossary}
\subsection{References}
\printbibliography[heading=none]

\subsection{Glossary}
\begin{acronym}[L-BFGS]
  \acro{BFGS}{Broyden-Fletcher-Goldfarb-Shanno algorithm}
  \acro{CNN}{Convolutional Neural Network}
  \acro{CTR}{Click through rate}
  \acro{GPU}{Graphics Processing Unit}
  \acro{L-BFGS}{Limited-memory BFGS}
  \acro{LSTM}{Long short-term memory}
  \acro{PCA}{Principal Component Analysis}
  \acro{RBM}{Restricted Boltzmann Machine}
  \acro{SGD}{Stochastic Gradient Descent}
  \acro{SIMD}{single instruction, multiple data}
  \acro{SVD}{Singular Value Decomposition}
  \acro{SVM}{Support Vector Machine}
\end{acronym}

\end{document}
